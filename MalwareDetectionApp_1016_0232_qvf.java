// 代码生成时间: 2025-10-16 02:32:23
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.ml.classification.LogisticRegression;
import org.apache.spark.ml.classification.LogisticRegressionModel;
import org.apache.spark.ml.feature.VectorAssembler;
import org.apache.spark.ml.feature.VectorIndexer;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.types.IntegerType;
import org.apache.spark.sql.types.StructType;

public class MalwareDetectionApp {

    public static void main(String[] args) {
        // Initialize Spark session
        SparkSession spark = SparkSession.builder()
                .appName("Malware Detection")
                .getOrCreate();

        // Create JavaSparkContext and SQL context
        JavaSparkContext sc = new JavaSparkContext(spark.sparkContext());

        // Load dataset
        String path = "path_to_your_dataset.csv";
        Dataset<Row> dataset = spark.read()
                .option("header", "true")
                .option("inferSchema", "true")
                .csv(path);

        // Define schema for the dataset
        StructType schema = new StructType()
                .add("features", new StructType()
                        .add("feature1", new IntegerType()))
                .add("label", new StructType());

        // VectorAssembler to combine multiple feature columns into a single vector column
        VectorAssembler assembler = new VectorAssembler()
                .setInputCols(new String[] {"feature1"})
                .setOutputCol("features");

        // Transform dataset to include vectorized features
        Dataset<Row> datasetWithFeatures = assembler.transform(dataset);

        // Split dataset into training and test sets (80% training, 20% test)
        Dataset<Row>[] datasetSplit = datasetWithFeatures.randomSplit(new double[]{0.8, 0.2});
        Dataset<Row> trainingData = datasetSplit[0];
        Dataset<Row> testData = datasetSplit[1];

        // Create a Logistic Regression model and train it with the training data
        LogisticRegression lr = new LogisticRegression()
                .setLabelCol("label")
                .setFeaturesCol("features");
        LogisticRegressionModel model = lr.fit(trainingData);

        // Evaluate the model using the test data
        Dataset<Row> predictions = model.transform(testData);
        double accuracy = evaluateModel(predictions);
        System.out.println("Model accuracy: " + accuracy);

        // Stop Spark session
        spark.stop();
    }

    // Function to evaluate the model accuracy
    private static double evaluateModel(Dataset<Row> predictions) {
        // Select (label, prediction) and compute accuracy
        Long totalCorrect = predictions.filter(functions.col("label").equalTo(functions.col("prediction"))).count();
        Long total = predictions.count();
        return (double) totalCorrect / total;
    }
}
